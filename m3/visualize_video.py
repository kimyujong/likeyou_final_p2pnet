"""
ì˜ìƒ ì‹œê°í™” ëª¨ë“ˆ

ì˜ìƒì„ ì¬ìƒí•˜ë©´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ëŒ ë””í…ì…˜ ë° í˜¼ì¡ë„ í‘œì‹œ
"""

import cv2
import os
import numpy as np
from typing import Optional
from dotenv import load_dotenv

from api import M3CongestionAPI
from utils import put_korean_text

load_dotenv()


def visualize_video_analysis(
    video_path: str,
    m3_api: M3CongestionAPI,
    frame_skip: int = 1,
    save_output: bool = False,
    output_path: Optional[str] = None,
    use_motion_filter: bool = True  # ë™ì‘ ì¸ì‹ í•„í„° ì˜µì…˜ ì¶”ê°€
):
    """
    ì˜ìƒì„ ì¬ìƒí•˜ë©´ì„œ ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    
    Args:
        video_path: ì˜ìƒ íŒŒì¼ ê²½ë¡œ
        m3_api: M3CongestionAPI ì¸ìŠ¤í„´ìŠ¤
        frame_skip: Ní”„ë ˆì„ë§ˆë‹¤ ë¶„ì„
        save_output: ê²°ê³¼ ì˜ìƒ ì €ì¥ ì—¬ë¶€
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        use_motion_filter: ì›€ì§ì„ì´ ì—†ëŠ” ë°°ê²½(ëŒì˜ì ë“±) ì˜¤íƒì§€ ì œê±° í™œì„±í™”
    """
    if not os.path.exists(video_path):
        print(f"âŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    # ì˜ìƒ ì—´ê¸°
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        print(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    # ì˜ìƒ ì •ë³´
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"\nğŸ“¹ ì˜ìƒ ì •ë³´:")
    print(f"  í•´ìƒë„: {width}x{height}")
    print(f"  FPS: {fps}")
    print(f"  ì´ í”„ë ˆì„: {total_frames}")
    
    # í‘œì‹œìš© í•´ìƒë„ ì„¤ì • (í™”ë©´ì— ë§ê²Œ ì¶•ì†Œ)
    display_width = 1280  # HD í•´ìƒë„ë¡œ í‘œì‹œ
    display_height = int(height * (display_width / width))
    print(f"  ğŸ“º í‘œì‹œ í¬ê¸°: {display_width}x{display_height}\n")
    
    # ì¶œë ¥ ì˜ìƒ ì„¤ì •
    out = None
    if save_output:
        if not output_path:
            output_path = video_path.replace('.', '_analyzed.')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        print(f"ğŸ’¾ ë¶„ì„ ì˜ìƒ ì €ì¥: {output_path}\n")
    
    # ë™ì‘ ê°ì§€ê¸° ì´ˆê¸°í™” (ë°°ê²½ í•™ìŠµìš©)
    bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, 
        varThreshold=25, 
        detectShadows=False
    ) if use_motion_filter else None
    
    print("ğŸ¬ ì˜ìƒ ì¬ìƒ ì‹œì‘! (ESC í‚¤ë¡œ ì¢…ë£Œ)\n")
    
    frame_count = 0
    last_result = None
    
    try:
        while True:
            ret, frame = cap.read()
            
            if not ret:
                break
            
            # ë™ì‘ ë§ˆìŠ¤í¬ ìƒì„± (ë§¤ í”„ë ˆì„ ì—…ë°ì´íŠ¸ í•„ìš”)
            motion_mask = None
            if bg_subtractor:
                # ì„±ëŠ¥ ìµœì í™”: ë™ì‘ ê°ì§€ëŠ” ì¶•ì†Œëœ ì´ë¯¸ì§€ì—ì„œ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)
                motion_scale = 640 / width
                motion_w = 640
                motion_h = int(height * motion_scale)
                
                frame_small = cv2.resize(frame, (motion_w, motion_h), interpolation=cv2.INTER_NEAREST)
                
                # ë°°ê²½ í•™ìŠµ ë° ë§ˆìŠ¤í¬ ìƒì„±
                mask_small = bg_subtractor.apply(frame_small)
                
                # ë…¸ì´ì¦ˆ ì œê±°
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                mask_small = cv2.morphologyEx(mask_small, cv2.MORPH_OPEN, kernel)
                
                # ë‹¤ì‹œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (ì¢Œí‘œ ë§¤í•‘ì„ ìœ„í•´)
                motion_mask = cv2.resize(mask_small, (width, height), interpolation=cv2.INTER_NEAREST)
            
            # ì›ë³¸ í”„ë ˆì„ ë³µì‚¬ (ê·¸ë¦¬ê¸°ìš©)
            display_frame = frame.copy()
            
            # frame_skipë§ˆë‹¤ ë¶„ì„
            if frame_count % frame_skip == 0:
                try:
                    # ì›ë³¸ í•´ìƒë„ ê·¸ëŒ€ë¡œ ë¶„ì„ (M3_origin ë°©ì‹)
                    result = m3_api.analyze_frame(frame)
                    
                    # [ìˆ˜ì •] í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì ìš©
                    # scoresê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    scores = result.get('scores', np.ones(len(result['points'])))
                    
                    # ë™ì‘ í•„í„° ì ìš©: ì›€ì§ì„ì´ ì—†ëŠ” ì˜ì—­ì˜ í¬ì¸íŠ¸ ì œê±°
                    if use_motion_filter and motion_mask is not None and len(result['points']) > 0:
                        filtered_points = []
                        
                        # ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸°
                        from constants import MOTION_CONFIRM_THRESHOLD
                        
                        for i, p in enumerate(result['points']):
                            score = scores[i] if i < len(scores) else 1.0
                            
                            # 1. ê³ ì‹ ë¢° ê°ì²´ (í™•ì‹  60% ì´ìƒ) -> ë¬´ì¡°ê±´ í†µê³¼ (ê°€ë§Œíˆ ìˆì–´ë„ ì¸ì •)
                            if score >= MOTION_CONFIRM_THRESHOLD:
                                filtered_points.append(p)
                                continue
                                
                            # 2. ì €ì‹ ë¢° ê°ì²´ (í™•ì‹  20%~60%) -> ì›€ì§ì„ ê²€ì¦ í•„ìš” (ëŒì˜ì ì œê±°)
                            x, y = int(p[0]), int(p[1])
                            # ì¢Œí‘œ ìœ íš¨ì„± ì²´í¬
                            if 0 <= x < width and 0 <= y < height:
                                # í•´ë‹¹ ì¢Œí‘œ ì£¼ë³€(5x5)ì— ì›€ì§ì„ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸
                                roi_motion = motion_mask[max(0, y-2):min(height, y+3), 
                                                         max(0, x-2):min(width, x+3)]
                                if np.sum(roi_motion) > 0:
                                    filtered_points.append(p)
                        
                        # í•„í„°ë§ëœ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸ (numpy arrayë¡œ ë³€í™˜)
                        result['points'] = np.array(filtered_points) if filtered_points else np.empty((0, 2))
                        result['count'] = len(result['points'])
                        
                        # ì¬ê³„ì‚°
                        result['density'] = m3_api.analyzer.calculate_density(result['count'])
                        result['pct'] = m3_api.analyzer.calculate_pct(result['count'])
                        result['risk_level'] = m3_api.analyzer.get_risk_level(result['pct'])
                    
                    last_result = result
                    
                except Exception as e:
                    print(f"âš ï¸ í”„ë ˆì„ {frame_count} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
            # ì‹œê°í™” (ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)
            if last_result:
                display_frame = draw_analysis_result(
                    display_frame,
                    last_result,
                    frame_count,
                    total_frames
                )
            
            # í™”ë©´ í‘œì‹œìš© í”„ë ˆì„ (ì¶•ì†Œ)
            display_frame_resized = cv2.resize(display_frame, (display_width, display_height))
            cv2.imshow('M3 P2PNet - CCTV Congestion Analysis', display_frame_resized)
            
            # ì¶œë ¥ ì˜ìƒ ì €ì¥ (ì›ë³¸ í¬ê¸°)
            if out:
                out.write(display_frame)
            
            # ESC í‚¤ë¡œ ì¢…ë£Œ
            key = cv2.waitKey(1) & 0xFF
            if key == 27:  # ESC
                print("\nâ¸ï¸ ì‚¬ìš©ìê°€ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
                break
            
            frame_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ (100í”„ë ˆì„ë§ˆë‹¤)
            if frame_count % 100 == 0:
                percent = (frame_count / total_frames) * 100
                print(f"  ì§„í–‰ë¥ : {percent:.1f}%")
    
    finally:
        cap.release()
        if out:
            out.release()
        cv2.destroyAllWindows()
    
    print(f"\nâœ… ì˜ìƒ ì²˜ë¦¬ ì™„ë£Œ: {frame_count}/{total_frames} í”„ë ˆì„")
    if save_output:
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")


def draw_analysis_result(
    frame: np.ndarray,
    result: dict,
    frame_number: int,
    total_frames: int
) -> np.ndarray:
    """
    í”„ë ˆì„ì— ë¶„ì„ ê²°ê³¼ ê·¸ë¦¬ê¸°
    """
    height, width = frame.shape[:2]
    
    # 1. ê²€ì¶œëœ ì‚¬ëŒ ìœ„ì¹˜ì— ì  í‘œì‹œ
    points = result.get('points', [])
    if len(points) > 0:
        for point in points:
            x, y = int(point[0]), int(point[1])
            
            # ì‹œê°ì  ë³´ì •: ì ì„ ì•½ê°„ ì•„ë˜ë¡œ ì´ë™ (ë¨¸ë¦¬ ìœ„ â†’ ì–¼êµ´/ëª¸í†µ)
            # ì£¼ì˜: ê³ ì •ê°’(100)ì€ ë©€ë¦¬ ìˆëŠ” ì‚¬ëŒì—ê²Œ ë„ˆë¬´ í½ë‹ˆë‹¤. ì¼ë‹¨ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì •í™•í•œ ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.
            y_offset = 0
            y = y + y_offset
            
            # ë¹¨ê°„ìƒ‰ ì›ìœ¼ë¡œ í‘œì‹œ (í¬ê¸° í™•ëŒ€: 5 -> 8)
            cv2.circle(frame, (x, y), 8, (0, 0, 255), -1)
            # í°ìƒ‰ í…Œë‘ë¦¬ (í¬ê¸° í™•ëŒ€: 6 -> 10)
            cv2.circle(frame, (x, y), 10, (255, 255, 255), 2)
    
    # 2. ìœ„í—˜ ë“±ê¸‰ì— ë”°ë¥¸ ìƒ‰ìƒ
    risk_level = result.get('risk_level')
    if hasattr(risk_level, 'color'):
        color = risk_level.color  # BGR
        level_text = risk_level.korean
    else:
        color = (0, 255, 0)  # ê¸°ë³¸ ë…¹ìƒ‰
        level_text = "ì•ˆì „"
    
    # 3. ìƒë‹¨ ì •ë³´ íŒ¨ë„ (ë°˜íˆ¬ëª… ë°°ê²½)
    overlay = frame.copy()
    panel_height = 180
    cv2.rectangle(overlay, (0, 0), (width, panel_height), (0, 0, 0), -1)
    frame = cv2.addWeighted(overlay, 0.6, frame, 0.4, 0)
    
    # 4. í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
    count = result.get('count', 0)
    pct = result.get('pct', 0)
    
    # ì¸ì› ìˆ˜
    text1 = f"ì¸ì›: {count}ëª…"
    frame = put_korean_text(frame, text1, (30, 30), font_size=50, color=(255, 255, 255))
    
    # í˜¼ì¡ë„
    text2 = f"í˜¼ì¡ë„: {pct:.1f}%"
    frame = put_korean_text(frame, text2, (30, 90), font_size=50, color=(255, 255, 255))
    
    # ë“±ê¸‰
    text3 = f"ë“±ê¸‰: {level_text}"
    frame = put_korean_text(frame, text3, (30, 150), font_size=40, color=color)
    
    # 5. í˜¼ì¡ë„ ê²Œì´ì§€ ë°”
    gauge_x = width - 350
    gauge_y = 30
    gauge_width = 300
    gauge_height = 30
    
    cv2.rectangle(frame, (gauge_x, gauge_y), (gauge_x + gauge_width, gauge_y + gauge_height), (100, 100, 100), -1)
    fill_width = int((pct / 100) * gauge_width)
    cv2.rectangle(frame, (gauge_x, gauge_y), (gauge_x + fill_width, gauge_y + gauge_height), color, -1)
    cv2.rectangle(frame, (gauge_x, gauge_y), (gauge_x + gauge_width, gauge_y + gauge_height), (255, 255, 255), 2)
    
    gauge_text = f"{pct:.0f}%"
    cv2.putText(frame, gauge_text, (gauge_x + gauge_width + 10, gauge_y + 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    # 6. ì§„í–‰ë¥ 
    progress_text = f"Frame: {frame_number}/{total_frames}"
    cv2.putText(frame, progress_text, (30, height - 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    # 7. ë“±ê¸‰ ì•„ì´ì½˜
    icon_size = 100
    icon_x = width - icon_size - 30
    icon_y = height - icon_size - 30
    
    cv2.rectangle(frame, (icon_x, icon_y), (icon_x + icon_size, icon_y + icon_size), color, -1)
    cv2.rectangle(frame, (icon_x, icon_y), (icon_x + icon_size, icon_y + icon_size), (255, 255, 255), 3)
    frame = put_korean_text(frame, level_text, (icon_x + 10, icon_y + 30), font_size=35, color=(255, 255, 255))
    
    return frame


if __name__ == "__main__":
    # GPU ì²´í¬ ë¡œì§ ì¶”ê°€
    import torch
    import sys
    
    print("="*70)
    print("ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ì ê²€")
    print(f"  - PyTorch ë²„ì „: {torch.__version__}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"  - âœ… GPU ê°ì§€ë¨: {gpu_name}")
        print(f"  - CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  - cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
    else:
        print("  - âŒ GPU(CUDA)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("  - âš ï¸  ì†ë„ê°€ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("  - í•´ê²°ì±…: PyTorchë¥¼ CUDA ë²„ì „ìœ¼ë¡œ ì¬ì„¤ì¹˜í•˜ì„¸ìš”.")
        # í™•ì¸ì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°í•˜ê±°ë‚˜ ì¢…ë£Œ
        # sys.exit(1) 
    print("="*70)

    # ì‹œê°í™” í…ŒìŠ¤íŠ¸
    print("ğŸ¬ M3 P2PNet ì˜ìƒ ë¶„ì„ ì‹œê°í™”")
    print("="*70)
    
    # í…ŒìŠ¤íŠ¸ ì˜ìƒ ê²½ë¡œ
    test_video = "C:/Users/user/m3_p2pnet/M3_dbtest/video/IMG_3579.mov"
    # test_video = "C:/Users/user/m3_p2pnet/M3_dbtest/video/IMG_3577.mov"
    # test_video = "C:/Users/user/m3_p2pnet/M3_dbtest/video/test_video.mp4"
    # test_video = "C:/Users/user/m3_p2pnet/M3_dbtest/video/IMG_3583_div.mp4"
    
    print("\nğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # ROI ì„¤ì • ì—†ì´ ìˆœìˆ˜ ì„±ëŠ¥ìœ¼ë¡œ íƒì§€
    # Threshold ì¡°ì •: 0.5 -> 0.35 (ë¯¼ê°ë„ í–¥ìƒ)
    from constants import DEFAULT_THRESHOLD
    m3_api = M3CongestionAPI(
        model_path=os.getenv('MODEL_PATH'),
        p2pnet_source_path=os.getenv('P2PNET_SOURCE'),
        device='cuda',
        max_capacity=200,
        alert_threshold=50,
        roi_polygon=None  # ROI ì œê±°
    )
    
    # ê°•ì œë¡œ ì„ê³„ê°’ ì¡°ì • (í•„ìš”ì‹œ)
    # m3_api.analyzer.threshold = 0.35
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
    
    if not os.path.exists(test_video):
        print(f"âŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_video}")
        print("ğŸ’¡ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        # ì˜ìƒ ì‹œê°í™” ì‹¤í–‰
        visualize_video_analysis(
            video_path=test_video,
            m3_api=m3_api,
            frame_skip=10,  
            save_output=True,
            output_path="C:/Users/user/m3_p2pnet/M3_dbtest/video_test_result/test_analyzed_motion5.mp4",
            use_motion_filter=True  # ë™ì‘ í•„í„° í™œì„±í™”
        )
        
        print("\nâœ… ì™„ë£Œ!")
        print("ğŸ’¡ ë¶„ì„ëœ ì˜ìƒì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ê²ƒì„ ì‹œì—°ì— ì‚¬ìš©í•˜ì„¸ìš”!")
